---
class: content
---

<div class="doc-header">
  <h1>「半分組み込まない」ロボコン</h1>
  <div class="doc-author">もるもるぷいかー</div>
</div>

# 「半分組み込まない」ロボコン

ソフトウェアの品質を競うETソフトウェアデザインロボットコンテスト (**ETロボコン**) では、ハードウェアの開発をほとんど行いません。 本章では、「半分組み込まない」ロボコンであるETロボコンについて、私が所属するチーム KatLab での開発体験を交えてご紹介いたします。

## ETロボコンとは

ロボットコンテスト (ロボコン) は各チーム、もしくは個人がロボットを作成し、それを用いて技術的な課題をクリアすることで競う競技です。
著名なロボコンに、アイデア対決・全国高等専門学校ロボットコンテスト (高専ロボコン) や、NHK学生ロボットコンテストなどがあります。これらのロボコンに限らず、一般的なロボコンのイメージから、「ロボコン = ハードウェア開発 + ソフトウェア開発」と認識している方が多いでしょう。

その中でもETロボコンは、世界的にもユニークな**ソフトウェア重視**の教育ロボコン[^ETロボコン公式サイト]です。
高校生以上であることを参加条件としています。プログラミング初経験の学生から、組み込み系で有名な企業所属の社会人まで幅広く参加しています。

[^ETロボコン公式サイト]: <https://www.etrobo.jp/>

ETロボコンは、ロボットに実際にコース上を走らせて得点を競う**走行部門**と、開発したソフトウェアを記述対象とした設計書の出来を競う**設計部門**があります。
総合成績は2つの成績を正規化して決定するため、良い成績を収めるには片方だけが突出しているだけでは不十分という、全国的にも珍しいロボコンです。

ETロボコンは、プログラミングや設計について入門者向けのエントリークラス、中級者向けのプライマリークラス、上級者向けのアドバンストクラスがあります。参加者は自らのレベルを考慮し、参加するクラスを決定します。

以降、私が参加しているプライマリークラスでの内容を記述いたします。

## ETロボコンの特徴

ETロボコンは、以下の特徴があります。
以降、ETロボコンにおいて用いるロボットを**走行体**、運営に提出する設計書を**モデル**と呼びます。

- 全チームで走行体が共通
- ソフトウェア開発が中心
- 総合成績が「走行」と「設計」で決定

### 全チームで走行体が共通

チームごとに特色あるロボットが動作する様子を見られるのは、ロボコンの醍醐味のひとつでしょう。
しかしETロボコンは、どのチームも共通の走行体を用いて課題を解決します。
市販の ETロボコン走行キット[^ETロボコン走行キット] という、レゴ® エデュケーション SPIKE™ プライムセット[^SPIKE] (以降 SPIKE) や、Raspberry Pi[^Raspberry_Pi] などを組み合わせたキットがあります。全チームがそのキットを組み立てて作成した走行体を走らせます。

[^ETロボコン走行キット]: <https://afrel.co.jp/product/et-set/>

[^SPIKE]: <https://education.lego.com/ja-jp/products/lego-education-spike-prime-set/45678/>

[^Raspberry_Pi]: <https://www.iodata.jp/product/pc/raspberrypi/ud-rp4b/spec.htm>

### ソフトウェア開発が中心

前述した通り、全チームで共通の走行体を走らせます。そのため、回路やアクチュエーターなどのハードウェア寄りの開発がほとんどありません。

ではどのように走行体を動かすかというと、SPIKE-RT[^SPIKE-RT] という RTOS (Real-Time Operating System) を SPIKE に搭載し、プログラミング言語 C++ によって SPIKE API [^SPIKE-API] を通じてモーターの駆動や、センサーでの測定値取得などを行います。
実際の開発環境では、ETロボコン運営が用意してくれている RasPike-ART[^RasPike-ART] という Raspberry Pi と SPIKE の制御開発環境を通じて API を叩き、制御します。

ETロボコンにおける各チームの開発対象は、走行体を制御するソフトウェアがほとんどです。
よって、一般的なロボコンを「ロボコン = ハードウェア開発 + ソフトウェア開発」とするならば、ETロボコンは「半分組み込まない」ロボコンと形容できるでしょう。

[^SPIKE-RT]: <https://github.com/spike-rt/spike-rt>

[^SPIKE-API]: <https://spike-rt.github.io/spike-rt/ja/html/modules.html>

[^RasPike-ART]: <https://github.com/ETrobocon/RasPike-ART>

### 総合成績が「走行」と「設計」で決定

ETロボコン最大の特徴のひとつは、設計が成績の半分を占めることです。
大規模化・複雑化するソフトウェア開発において、ソースコードのみを追跡して開発を進めることは困難です。そこで、設計によって、それをある視点で抽象化 (**モデリング**) し、大規模化するソフトウェアの全体像が把握しやすくなります。

ETロボコンでは、開発対象である走行体を制御するソフトウェアについて、モデリングした設計書であるモデルを運営に提出する必要があります。
いわゆる UML (Unified Modeling Language) や SysML (Systems Modeling Language) といったモデリング言語を用います。

ETロボコンは、1年に地区大会とチャンピオンシップ大会 (全国大会) の計2回の大会が開催されます。
両大会において、走行部門と設計部門がそれぞれ評価され、総合成績が決まります。

## 大会の詳細とチーム KatLab の攻略方法

本章では、大会の詳細を2024年度開催のETロボコン2024の内容のうち、走行部門に関することを 3 つと、モデルについてを紹介いたします。
同時に、私が所属するチーム KatLab での攻略方法を紹介いたします。

なお、記述する図などは、ETロボコン実行委員会が配布している、ETロボコン2024競技規約 1.0.2 版[^ETロボコン公式規約]、および、ETロボコン難所組立図 1.0.0 版[^ETロボコン難所攻略図]から引用いたします。

[^ETロボコン公式規約]: <https://docs.etrobo.jp/rules/2024/ETRC2024_rules_primary_advanced_1.0.2.pdf>

[^ETロボコン難所攻略図]: <https://docs.etrobo.jp/rules/2024/ETRC2024_nansyo_1.0.0.pdf>

ETロボコン2024で走行体が走るコースの全景を、図1に示します。
図1左半分の L コースと右半分の R コースからなり、各コースにつき 1 回、計 2 回のうち良い方の成績を採用します。

![コースの全景](./images_molpui/course.png 'コースの全景')

走行部門ではポイント獲得対象として、走行ポイントとボーナスポイントが獲得できます。
走行部門のポイントは、走行ポイントとボーナスポイントの合計で算出されます。

走行ポイントは、スタートから特定位置にあるラップゲートを通過するまでにかかる時間によって入るポイントです。
ラップゲート通過までの時間を短縮できるほど、獲得できる走行ポイントは高くなります。

ボーナスポイントは、ラップゲート通過や難所攻略など、ある特定の条件を達成した際に獲得できるポイントです。
難所の攻略状況によって、獲得できるポイントは上下する場合がします。

なお、ポイントを算出する式や難所あたりで獲得できるポイントなどについては省略いたします。

### 走行体の制御

KatLab は走行体を走らせるため、ライントレースを採用しました。
L コースについては、黒線の左境界を中心として、走行体下部にあるカラーセンサーが白色を検知すると右へ、黒色を検知すると左へ進むようにしました。
これにより、走行体は黒線の左境界を中心にジグザグと前方に進みます。

### プラレール撮影

2つの円のうち真円のエリアでは、真円の内側を周回する3両のプラレールと、四角柱の側面ひとつにある背景を収めて画像を撮影することでボーナスポイントを獲得できます。
背景のみ、プラレールのみの撮影でも一定のポイントは獲得できます。
しかし、背景とプラレール3両側面を同時に撮影することで、獲得できるポイントが高くなります。
プラレール撮影を攻略している画像を図 2 に示します。

![プラレール撮影攻略中の走行体](./images_molpui/snap_plarail.png 'プラレール撮影攻略中の走行体')

KatLab は動画の録画と動体検知によって上限の点数を獲得できました。
背景の方向は当日の本番前に決定します。撮影箇所を6箇所パラメータとして持たせ、走行前にパラメータを設定することで背景を撮影しました。
そこで、プラレール3両側面を撮影するため、プラレールがカメラを通過する区間の中間のフレームを特定して切り出す方法を採用しました。

具体的には、以下のように実装しました。

1. 背景をカメラ正面で捉える
1. カメラ中央にバウンディングボックスを配置する
1. 録画を開始する
1. プラレールがバウンディングボックスに侵入したタイミングを計測する
1. プラレールがバウンディングボックスを退出したタイミングを計測する
1. 計測したバウンディングボックス侵入・退出タイミングから中間のフレームを切り出す

難所組立図には本番用いるプラレールの画像がなかった[^ETロボコン難所組立図]。そのため、機械学習による実装方法は、学習するプラレールによってはうまく検出できない場合があると考え、画像処理で実装しました。

### ミニフィグ撮影

2つの円のうち楕円のエリアでは、中央にあるミニフィグの画像を撮影することでボーナスポイントを獲得できます。
ミニフィグ全身と台座が映っていれば、どの方向から撮影しても一定のポイントを獲得できます。
しかし、ミニフィグ正面 (両目、口が判別できる状態) から撮影することで、獲得できるポイントが高くなります。
ミニフィグ撮影を攻略している際の走行体の様子を図 3 に示します。

![ミニフィグ撮影攻略中の走行体](./images_molpui/snap_fig.png 'ミニフィグ撮影攻略中の走行体')

KatLab は機械学習と画像処理によって上限の点数を獲得できました。
プラレール撮影の背景とは異なり、ミニフィグの向いている方向は走行直前に決まります。このタイミングは PC にも走行体にも触れられないため、パラメータとして設定することはできません。
そこで、機械学習によってミニフィグが向いている方向を特定する方法を採用しました。

具体的には、以下のように実装しました。

1. ミニフィグの向きを特定するため、事前に決定していた箇所からミニフィグを撮影する
1. 1. で撮影した画像から、ミニフィグの向き (正面、右、左、後ろ) 機械学習でを特定する
1. 特定した方向が正面である場合、1. で撮影した画像を提出する
1. 特定した方向が正面でない場合、特定した方向に応じた撮影箇所へ移動し、ミニフィグを撮影する

実際には、3. の条件に入った、すなわち向きを特定するために撮影した 1. の画像を提出した場合は、4. の移動と撮影は不要になるため、スキップします。
また、なんらかの理由でミニフィグの向きを判定できなかった場合は、右、左、後ろどの方向を向いていてもよいように 4 つの箇所から画像を撮影し、機械学習で正面らしさのスコアをそれぞれ算出します。その後、 4 枚の中から、最も正面らしいスコアが高い画像を提出するようにしました。

### モデル

モデルは以下の 4 つで構成します。全体の構成は全チーム共通です。

1. 要求モデル
1. 分析モデル
1. 設計モデル
1. 制御モデル

## 2024年度のチーム KatLab での体験

### 大会成績

### チームリーダーの経験

### 感想と2025年度の目標

## おわりに
