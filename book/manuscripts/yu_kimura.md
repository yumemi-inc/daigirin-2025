---
class: content
---

<div class="doc-header">
  <h1>「半分組み込まない」ロボコン</h1>
  <div class="doc-author">もるもるぷいかー</div>
</div>

# 「半分組み込まない」ロボコン

ソフトウェアの品質を競う ET ソフトウェアデザインロボットコンテスト (**ETロボコン**) では、ハードウェアの開発をほとんど行いません。 本章では、「半分組み込まない」ロボコンである ET ロボコンについて、私が所属するチーム KatLab での開発体験を交えてご紹介いたします。

## ETロボコンとは

ロボットコンテスト（ロボコン）は各チーム、もしくは個人がロボットを作成し、それを用いて技術的な課題をクリアすることで競う競技です。
著名なロボコンに、アイデア対決・全国高等専門学校ロボットコンテスト (高専ロボコン) や、NHK 学生ロボットコンテストなどがあります。これらのロボコンに限らず、一般的なロボコンのイメージから、「ロボコン = ハードウェア開発 + ソフトウェア開発」と認識している方が多いでしょう。

その中でも ET ロボコンは、世界的にもユニークな**ソフトウェア重視**の教育ロボコン[^ETロボコン公式サイト]です。
高校生以上であることを参加条件としています。プログラミング初経験の学生から、組み込み系で有名な企業所属の社会人まで幅広く参加しています。

[^ETロボコン公式サイト]: ET ロボコン, 組込みソフトウェア技術教育をテーマとした「ET ロボコン」, <https://www.etrobo.jp/>, 最終アクセス日：2025/05/01

ET ロボコンは、ロボットに実際にコース上を走らせて得点を競う**走行部門**と、開発したソフトウェアを記述対象とした設計書の出来を競う**設計部門**があります。
総合成績は 2 つの成績を正規化して決定するため、好成績を収めるには片方だけが突出しているだけでは不十分という、全国的にも珍しいロボコンです。

ET ロボコンは、プログラミングや設計について入門者向けのエントリークラス、中級者向けのプライマリークラス、上級者向けのアドバンストクラスがあります。参加者は自らのレベルを考慮し、参加するクラスを決定します。

以降、私が参加しているアドバンストクラスでの内容を記述いたします。

## ETロボコンの特徴

ET ロボコンは、次の特徴があります。
以降、ET ロボコンにおいて用いるロボットを**走行体**、運営に提出する設計書を**モデル**と呼びます。

- 全チームで走行体が共通
- ソフトウェア開発が中心
- 総合成績が「走行」と「設計」で決定

### 全チームで走行体が共通

チームごとに特色あるロボットが動作する様子を見られるのは、ロボコンの醍醐味のひとつでしょう。
しかし ET ロボコンは、どのチームも共通の走行体を用いて競技に参加します。
市販の ET ロボコン走行キット[^ETロボコン走行キット] という、レゴ® エデュケーション SPIKE™ プライムセット[^SPIKE]（以降 SPIKE）や、Raspberry Pi[^Raspberry_Pi] などを組み合わせたキットがあります。全チームがそのキットを組み立てて作成した走行体を走らせます。

[^ETロボコン走行キット]: Afrel, ET ロボコン走行キット<https://afrel.co.jp/product/et-set/>, 最終アクセス日：2025/05/01

[^SPIKE]: LEGO® Education, レゴ® エデュケーション SPIKE™ プライムセット, <https://education.lego.com/ja-jp/products/lego-education-spike-prime-set/45678/>, 最終アクセス日：2025/05/01

[^Raspberry_Pi]: アイ・オー・データ機器｜I-O DATA, Raspberry Pi 4 Model B（UD-RP4B シリーズ）仕様, <https://www.iodata.jp/product/pc/raspberrypi/ud-rp4b/spec.htm>, 最終アクセス日：2025/05/01

### ソフトウェア開発が中心

前述したとおり、全チームで共通の走行体を走らせます。
走行体はキットの組み立てで完成し、回路やアクチュエーターなどのハードウェア寄りの開発がほとんどありません。

走行体は、SPIKE-RT[^SPIKE-RT] という RTOS（Real-Time Operating System）を SPIKE に搭載しています。
プログラミング言語 C++ によって SPIKE API [^SPIKE-API] を通じてモーターの駆動や、センサーでの測定値取得などを行います。
実際の開発環境では、ET ロボコン運営が用意する RasPike-ART[^RasPike-ART] という Raspberry Pi と SPIKE の制御開発環境を通じて API を叩き、制御します。

ET ロボコンにおける各チームの開発対象は、走行体を制御するソフトウェアがほとんどです。
よって、一般的なロボコンを「ロボコン = ハードウェア開発 + ソフトウェア開発」とするならば、ET ロボコンは「半分組み込まない」ロボコンと形容できるでしょう。

[^SPIKE-RT]: GitHub, spike-rt, <https://github.com/spike-rt/spike-rt>, 最終アクセス日：2025/05/01

[^SPIKE-API]: SPIKE-RT C API Reference[Japanese], <https://spike-rt.github.io/spike-rt/ja/html/modules.html>, 最終アクセス日：2025/05/01

[^RasPike-ART]: GitHub, RasPike-ART, <https://github.com/ETrobocon/RasPike-ART>, 最終アクセス日：2025/05/01

### 総合成績が「走行」と「設計」で決定

ET ロボコン最大の特徴のひとつは、設計が成績の半分を占めることです。
大規模化・複雑化するソフトウェア開発において、ソースコードのみを追跡して開発を進めることは困難です。そこで、設計によって、それをある視点で抽象化 (**モデリング**) し、大規模化するソフトウェアの全体像が把握しやすくなります。

ET ロボコンでは、開発対象である走行体を制御するソフトウェアについて、モデリングした設計書であるモデルを運営に提出する必要があります。
いわゆる UML（Unified Modeling Language）や SysML（Systems Modeling Language）といったモデリング言語を用います。

ET ロボコンは、1 年に地区大会とチャンピオンシップ大会（全国大会）の計 2 回の大会が開催されます。
両大会において、走行部門と設計部門がそれぞれ評価され、総合成績が決まります。

## 大会の詳細とチーム KatLab の攻略方法

本章では、大会の詳細を 2024 年度開催の ET ロボコン 2024 の内容のうち、走行部門に関することを 3 つと、モデルについてを紹介いたします。
同時に、私が所属するチーム KatLab での攻略方法を紹介いたします。

なお、記述する図などは ET ロボコン実行委員会が配布している次の資料から引用いたします。

- ET ロボコン 2024 競技規約 1.0.2 版[^公式規約]
- ET ロボコン 2024 難所組立図 1.0.0 版[^難所攻略図]
- ET ロボコン 2024 審査規約 1.0 版[^審査規約]

[^公式規約]: ET ロボコン実行委員会, ET ロボコン 2024 競技規約 1.0.2 版, <https://docs.etrobo.jp/rules/2024/ETRC2024_rules_primary_advanced_1.0.2.pdf>, 最終アクセス日：2025/05/01

[^難所攻略図]: ET ロボコン実行委員会, ET ロボコン 2024 難所組立図 1.0.0 版, <https://docs.etrobo.jp/rules/2024/ETRC2024_nansyo_1.0.0.pdf>, 最終アクセス日：2025/05/01

[^審査規約]: ET ロボコン実行委員会, ET ロボコン 2024 審査規約 1.0 版, <https://docs.etrobo.jp/rules/2024/ETRC2024_dev_shinsa_rules_V1.0.pdf>, 最終アクセス日：2025/05/01

### 走行部門

ET ロボコン 2024 で走行体が走るコースの全景を、図 1 に示します。
図 1 左半分の L コースと右半分の R コースからなり、各コースにつき 1 回、計 2 回のうち高い得点の方を採用します。

![コースの全景](./images_yu_kimura/course.png 'コースの全景')

走行部門ではポイント獲得対象として、走行ポイントとボーナスポイントが獲得できます。
走行部門のポイントは、走行ポイントとボーナスポイントの合計で算出されます。

走行ポイントは、スタート（図 1 中の①）からラップゲート（図 1 中の②）を通過するまでにかかる時間によって入るポイントです。
ラップゲート通過までの時間を短縮できるほど、獲得できる走行ポイントは高くなります。

ボーナスポイントは、ラップゲート通過や難所攻略など、ある特定の条件を達成した際に獲得できるポイントです。
難所の攻略状況によって、獲得できるポイントは上下する場合があります。

なお、ポイントを算出する式や難所あたりで獲得できるポイントなどについては省略いたします。

#### 走行体の制御

KatLab は走行体を走らせるため、ライントレースを採用しました。
L コースについては、黒線の左境界を中心として、走行体下部にあるカラーセンサーが白色を検知すると右へ、黒色を検知すると左へ進むようにしました。
これにより、走行体は黒線の左境界を中心にジグザグと前方に進みます。

#### プラレール・背景撮影

2 つの円のうち真円のエリア（図 1 中の③）では、真円の内側を周回する 3 両のプラレールと、四角柱の側面ひとつにある背景を収めて画像を撮影することでボーナスポイントを獲得できます。
背景のみ、プラレールのみの撮影でも一定のポイントは獲得できます。
しかし、背景とプラレール 3 両側面を同時に撮影することで、獲得できるポイントが高くなります。
プラレール・背景撮影を攻略している走行体の様子を、図 2 に示します。

![プラレール・背景撮影攻略中の走行体](./images_yu_kimura/snap_plarail.png 'プラレール・背景撮影攻略中の走行体')

KatLab は動画の録画と動体検知によって上限の点数を獲得できました。
背景の方向は当日の本番前に決定します。撮影箇所を 6 箇所パラメータとして持たせ、走行前にパラメータを設定することで背景を撮影しました。
そこで、プラレール 3 両側面を撮影するため、プラレールがカメラを通過する区間の中間のフレームを特定して切り出す方法を採用しました。

具体的には、次のように実装しました。

1. 背景をカメラ正面で捉える
1. カメラ中央にバウンディングボックスを配置する
1. 録画を開始する
1. プラレールがバウンディングボックスに侵入したタイミングを計測する
1. プラレールがバウンディングボックスを退出したタイミングを計測する
1. 計測したバウンディングボックス侵入・退出タイミングから中間のフレームを切り出す

難所組立図には、本番に用いるプラレールの画像がありませんでした。
そのため、機械学習による実装では学習するプラレールによってはうまく検出できない場合があると考え、画像処理で実装しました。
バウンディングボックスにプラレールが侵入したときのフレーム、退出したときのフレーム、中間のフレームを、図 3 に示します。

![中間フレームの切り出し](./images_yu_kimura/snap_plarail_image.png '中間フレームの切り出し')

#### ミニフィグ撮影

2 つの円のうち楕円のエリア（図 1 中の④）では、中央にあるミニフィグの画像を撮影することでボーナスポイントを獲得できます。
ミニフィグ全身と台座が映っていれば、どの方向から撮影しても一定のポイントを獲得できます。
しかし、ミニフィグ正面（両目、口が判別できる状態）から撮影することで、獲得できるポイントが高くなります。
ミニフィグ撮影を攻略している走行体の様子を、図 4 に示します。

![ミニフィグ撮影攻略中の走行体](./images_yu_kimura/snap_fig.png 'ミニフィグ撮影攻略中の走行体')

KatLab は機械学習と画像処理によって上限の点数を獲得できました。
プラレール撮影の背景とは異なり、ミニフィグの向いている方向は走行直前に決まります。このタイミングは PC と走行体には触れられないため、パラメータとして設定できません。
また、使用するミニフィグの種類は、難所組立図に記載されていました。
そこで、機械学習によってミニフィグが向いている方向を特定する方法を採用しました。

具体的には、次のように実装しました。

1. ミニフィグの向きを特定するため、事前に決定していた箇所からミニフィグを撮影する
1. 無線接続している PC へ走行体から撮影した画像を送信する
1. 1. で撮影した画像から、ミニフィグの向き（正面、右面、左面、背面）を機械学習で特定する
1. 特定した方向が正面である場合、1. で撮影した画像を提出する
1. 特定した方向が正面でない場合、特定した方向に応じた撮影箇所へ移動し、ミニフィグを撮影する

YOLO v5 を用いてミニフィグの画像を物体検知し、正面、右面、左面、背面の 4 つから判定します。
当初は走行体の Raspberry Pi で推論する予定でしたが、Raspberry Pi OS が 32 bit でした。
YOLO の環境構築時、Raspberry Pi OS が 64 bit でないと推論できないことに気づきました。
そこで、走行体へ無線通信している PC へ画像を送信し、PC で推論する方針に切り替えました。

また、3. の条件に入った、すなわち向きを特定するために撮影した 1. の画像を提出した場合は、4. の移動と撮影は不要になるため、処理をスキップします。
また、なんらかの理由でミニフィグの向きを判定できなかった場合は、右、左、後ろどの方向を向いていてもよいように 4 つの箇所から画像を撮影し、機械学習で正面らしさのスコアをそれぞれ算出します。その後、 4 枚の中から、もっとも正面らしいスコアが高い画像を提出するようにしました。
ミニフィグの向き判定成功時の画像を、図 5 に示します。

![ミニフィグ向き判定（左から正面、右面、背面、左面）](./images_yu_kimura/predict_fig_direction.png 'ミニフィグの向き判定')

### 設計部門

モデルは走行部門と異なり、詳細な点数が開示されません。
評価は A から D までの 4 段階に分かれており、それぞれに+と-の記号が付くことで、合計 12 段階の細かい評価が行われます。

モデルは次の 4 つで構成します。全体の構成は全チーム共通です。

1. 要求モデル
1. 分析モデル
1. 設計モデル
1. 制御モデル

#### 要求モデル

要求モデルでは、開発の目標と、それを達成するために必要な機能、機能に付随する品質や制約の検討がされているかが見られます。

KatLab が書いた要求モデルを図 6 に示します。
KatLab は、開発目標と目標得点を決め、開発目標の達成のためのユースケースを定義し、それをもとに導出した要求と要素技術を要求図に起こしました。

![KatLab の要求モデル](./images_yu_kimura/model/1_requirements.png 'KatLab の要求モデル')

ミニフィグ撮影を例に挙げます。
ミニフィグ撮影でミニフィグを正面から撮影することを目標として、カメラによる画像撮影の機能や、ミニフィグを正面から撮影した画像を提出する機能などが必要。
ミニフィグ正面を正面から撮影した画像を提出するために、ミニフィグが正面を向いているかを判定する機能が必要、と要求を上位から下位へと分解します。

このとき、通信や撮影などが失敗した際の動作など（非機能要件）を考慮する必要もあります。
ミニフィグ正面方向の判定が失敗した際は、4 つの画像から正面らしさのスコアを算出して比較し、もっともスコアの高い画像を提出するという要求は非機能要求にあたります。
この要求がない場合は、失敗そのものと失敗時の動作を考慮しないことを示します。
この場合、品質特性における信頼性（正しく機能できる度合、異常発生時の回復のしやすさ）が足りていないため、減点されます。

#### 分析モデル

分析モデルでは、導出した要求や制約に基づくシステム全体の構造や動作の分析ができているかが見られます。

KatLab が書いた分析モデルを図 7 に示します。
KatLab は、競技規約からシステム分析による静的構造を導出し、コンテキスト分析で ET ロボコン全体、走行体内部、それぞれの間での関係性を示すことでソフトウェアの開発対象を示しました。
導出した要求と開発対象から、要素技術を構成するサブシステムを定義し、その静的振る舞いを示しました。

![KatLab の分析モデル](./images_yu_kimura/model/2_analysis.png 'KatLab の分析モデル')

プラレール・背景撮影を例に挙げます。
静的構造を導出し、ソフトウェアの開発対象を明確化しました。
要求モデルにおいて、プラレール・背景撮影で背景が本番当日に決まるという制約から、撮影箇所事前設定の要素技術を導出しました。
サブシステムとして、走行全体の制御に関する走行システムと、走行体が走行開始する前に撮影箇所をパラメータとして受け取る撮影箇所指定システムをそれぞれ定義しました。

#### 設計モデル

設計モデルでは、システム全体の分析を元に要求を実現する各システムの構造と振舞いの設計が見られます。

KatLab が書いた設計モデルを図 8、図 9 にそれぞれ示します。
KatLab は、要求モデルと分析モデルから注意事項を抽出し、設計方針を決定しました。
設計方針から、開発システム全体の依存関係をパッケージ図で示し、アーキテクチャを決定しました（図 8）。

![KatLab の分析モデル（ 1 枚目）](./images_yu_kimura/model/3_design1.png 'KatLab の分析モデル（ 1 枚目）')

また、アーキテクチャを考慮しつつクラス図によって各サブシステムの内部構造と関連性を記述し、状態遷移図とアクティビティ図を用いて動的振る舞いを示しました（図 9）。

![KatLab の分析モデル（ 2 枚目）](./images_yu_kimura/model/3_design2.png 'KatLab の分析モデル（ 2 枚目）')

プラレール・背景撮影、および、ミニフィグ撮影に関して、Facade（ファサード）パターンを例に挙げます。
開発効率に関して拡張性を向上するという上位要求から、パッケージ間を疎結合にするという下位要求を導出し、Facad パターンの設計方針を要求モデルで導出しました。
Facade パターンを実現するため、今年の競技固有の機能を束ねたパッケージと、汎用的な機能を束ねたパッケージに分解しました。
たとえば、カメラを用いて画像や動画を撮影する処理は、汎用的な処理にあたります。
対して、プラレールのバウンディングボックス侵入・退出タイミングを用いた中間フレーム切り出しの処理や、ミニフィグの向きを特定する処理は、今年の競技固有の処理にあたります。
設計方針において、カメラに関するサブシステムであるフロントカメラシステムは、次の 4 つのパッケージに分解しました。

- カメラインターフェース
- 動体検出部
- 画像解析部
- カメラ制御部

動体検出部と画像解析部は、プラレールの中間フレーム切り出しや、ミニフィグの向き判定などの競技固有の処理をそれぞれ含んでいます。
カメラ制御部は、画像や動画の撮影などの汎用的な処理を含んでいます。
動体検出部、画像解析部はカメラ制御部の public な関数を呼び出しています。
さらに動体検出部と画像解析部の各パッケージにおける処理は、カメラインターフェースで定義した public な関数を経由して呼び出されます。
これによって、フロントカメラシステムにおける保守性と拡張性の向上という非機能要求を実現可能な設計に落とし込みます。

#### 制御モデル

制御モデルは、要求で定義した品質を満たすための制御戦略と、その戦略で用いられる要素技術の検討内容と結果が見られます。

KatLab が書いた制御モデルを、図 10 に示します。
KatLab は、走行全体、プラレール・背景撮影、ミニフィグ撮影について制御戦略が要求を満たすことを確認しました。
走行体の動作を細分化し、図を含めて制御戦略の説明を示しました。
詳細なデータを示せる場合は具体的に、成功率 ◯◯ ％ 達成などと数値を用いて、採用した制御戦略が実現可能であることを定量的に評価しました。

![KatLab の制御モデル](./images_yu_kimura/model/4_control.png 'KatLab の制御モデル')

制御モデルは各チームによってかなり差があります。
KatLab は代々、ユニークなツールを作成したことを書いています。
今年は、走行ログ可視化ツールを Web アプリとして作成したことで、走行動作が失敗する原因の特定に寄与したと、走行全体の制御戦略のひとつとして書きました。
昨年は、パラメータ指定ツールを iOS アプリとして作成したことで、調整にかかる時間を削減できたと書きました。

## 2024年度のチーム KatLab での体験

本章では、チーム KatLab で得られた体験について書きます。

### 大会成績

九州地区大会でモデル 1 位、総合 2 位。
チャンピオンシップ大会（全国大会）で総合 5 位の成績を収めました。
KatLab は 2018 年から 7 年連続で全国出場を果たしていたため、その記録を継続させることができました。

### チームリーダーの経験

昨年と同じく、開発方式はアジャイル開発で進めました。
モデルを書くのであればウォーターフォール開発が向いていそうとよくいわれますが、これはイテレーションごとに動作するものを作り上げられるためです。
ウォーターフォール開発は、未完成の段階では動作が保証できません。
一方でアジャイル開発は、イテレーションごとに動くものを細かく作成します。
ET ロボコンのコースは、難所の攻略順がおおよそ決まっています。
そのため、途中で動作が止まる、コースアウトするなどで復帰不能となった場合は、以降の得点がすべて失われます。
極端な話、ラップゲート通過前にコースアウトすると、ボーナスポイントはもちろん走行ポイントも入りません。
まず基礎的な動作を完成させ、次はこの難所攻略、次はこの難所攻略、と段階を踏んで開発することで、開発が間に合わなかった場合に大きく失点するリスクを防いでいます。

マネジメント関連は、常に自分からコミュニケーションを取りにいくことを意識しました。
タスクで困っていることがないか、実装方法がミーティング時から変わったことがないかを積極的に確認していました。
ただ、すべてを把握しようとすると窮屈に思われると考えたため、タスクにつき 1 回や 2 回と意識的に抑えていました。
メンバーが全員優秀だったこともあり、明らかに困っている様子でない限り、タスクごとに分けたグループ内で解決してもらう方針を取りました。
そのため、学生プログラミング団体でありがちな、一部の優れた人間が開発を先導する状況にはなりませんでした。
これは、平等に学習の機会があったという意味でよい効果があったと考えています。

### 感想と2025年度の目標

学生のうちから設計に関わることはかなり珍しいと思っています。
それだけに、今しか集まれないメンバーと足並みを揃えて、今しかできない ET ロボコンで得た経験は、自分を形成する上で欠かせないものだと強く考えています。
こうしてゆめみに内定をもらったり、ゆめみ大技林に寄稿することもなかったことでしょう。
優秀なメンバーに囲まれ、リーダーとして特別にやったことはさほどなかったと記憶しています。
いくつも失敗しましたが、それでもメンバーのみんながフォローしてくれ、最後まで開発できたので大変満足しています。
メンバーのみんなには本当に頭が上がりません。

今年度も全国大会にて総合 3 位以内で入賞の目標を継続します。
KatLab は 2018 年から 7 年連続で全国出場を果たしているものの、全国での成績が停滞気味です。
今年度こそ、新生 KatLab メンバーで全国入賞を果たします。

## まとめ

まず、ゆめみ大技林の執筆について勧誘をしていただきました、うーたん様にお礼申し上げます。
ゆめみ大技林の執筆者の方々にも、レビュー・推敲をしていただきました。ありがとうございます。
KatLab のチームメンバーの皆さんも、推敲を手伝ってくださりありがとうございました。
昨年活動分の感謝と一緒に、今年の ET ロボコンの活動で返します。

最後に、ここまで閲覧いただきありがとうございます。
アプリというより組み込み、プログラムというより設計と、他の執筆者の方々とは少し外れたテーマでしたが、だからこそ希少性のあるテーマで寄稿できたと考えています。

本文で用いたモデルの画像は、Google Drive に共有フォルダとしてまとめました。
拡大してモデル画像を見たい方は、次のリンク、もしくは QR コードからご覧ください。
モデルの画像をまとめた Google Drive の QR コードを、図 11 に示します。

```url
https://drive.google.com/drive/folders/1r0xD7jiTnK6KrH1i7InL1uyFjWCyJ-F2?usp=drive_link
```

![モデルの画像を集めた Google Drive の QR コード](./images_yu_kimura/google_drive_qr_code.png 'モデルの画像を集めた Google Drive の QR コード')
