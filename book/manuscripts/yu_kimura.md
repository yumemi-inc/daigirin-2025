---
class: content
---

<div class="doc-header">
  <h1>「半分組み込まない」ロボコン</h1>
  <div class="doc-author">もるもるぷいかー</div>
</div>

# 「半分組み込まない」ロボコン

ソフトウェアの品質を競う ET ソフトウェアデザインロボットコンテスト (**ETロボコン**) では、ハードウェアの開発をほとんど行いません。 本章では、「半分組み込まない」ロボコンである ET ロボコンについて、私が所属するチーム KatLab での開発体験を交えてご紹介いたします。


## ETロボコンとは

ロボットコンテスト（ロボコン）は各チーム、もしくは個人がロボットを作成し、それを用いて技術的な課題をクリアすることで競う競技です。
著名なロボコンに、アイデア対決・全国高等専門学校ロボットコンテスト (高専ロボコン) や、NHK 学生ロボットコンテストなどがあります。これらのロボコンに限らず、一般的なロボコンのイメージから、「ロボコン = ハードウェア開発 + ソフトウェア開発」と認識している方が多いでしょう。

その中でも ET ロボコンは、世界的にもユニークな**ソフトウェア重視**の教育ロボコン[^ETロボコン公式サイト]です。
高校生以上であることを参加条件としています。プログラミング初経験の学生から、組み込み系で有名な企業所属の社会人まで幅広く参加しています。

[^ETロボコン公式サイト]: ET ロボコン, 組込みソフトウェア技術教育をテーマとした「ET ロボコン」, <https://www.etrobo.jp/>, 最終アクセス日：2025/05/01

ET ロボコンは、ロボットに実際にコース上を走らせて得点を競う**走行部門**と、開発したソフトウェアを記述対象とした設計書の出来を競う**設計部門**があります。
総合成績は 2 つの成績を正規化して決定するため、好成績を収めるには片方だけが突出しているだけでは不十分という、全国的にも珍しいロボコンです。

ET ロボコンは、プログラミングや設計について入門者向けのエントリークラス、中級者向けのプライマリークラス、上級者向けのアドバンストクラスがあります。参加者は自らのレベルを考慮し、参加するクラスを決定します。

以降、私が参加しているアドバンストクラスでの内容を記述いたします。


## ETロボコンの特徴

ET ロボコンは、次の特徴があります。
以降、ET ロボコンにおいて用いるロボットを**走行体**、運営に提出する設計書を**モデル**と呼びます。

- 全チームで走行体が共通
- ソフトウェア開発が中心
- 総合成績が「走行」と「設計」で決定


### 全チームで走行体が共通

チームごとに特色あるロボットが動作する様子を見られるのは、ロボコンの醍醐味のひとつでしょう。
しかし ET ロボコンは、どのチームも共通の走行体を用いて競技に参加します。
市販の ET ロボコン走行キット[^ETロボコン走行キット] という、レゴ® エデュケーション SPIKE™ プライムセット[^SPIKE]（以降 SPIKE）や、Raspberry Pi[^Raspberry_Pi] などを組み合わせたキットがあります。全チームがそのキットを組み立てて作成した走行体を走らせます。

[^ETロボコン走行キット]: Afrel, ET ロボコン走行キット<https://afrel.co.jp/product/et-set/>, 最終アクセス日：2025/05/01

[^SPIKE]: LEGO® Education, レゴ® エデュケーション SPIKE™ プライムセット, <https://education.lego.com/ja-jp/products/lego-education-spike-prime-set/45678/>, 最終アクセス日：2025/05/01

[^Raspberry_Pi]: アイ・オー・データ機器｜I-O DATA, Raspberry Pi 4 Model B（UD-RP4B シリーズ）仕様, <https://www.iodata.jp/product/pc/raspberrypi/ud-rp4b/spec.htm>, 最終アクセス日：2025/05/01


### ソフトウェア開発が中心

前述したとおり、全チームで共通の走行体を走らせます。
走行体はキットの組み立てで完成し、回路やアクチュエーターなどのハードウェア寄りの開発がほとんどありません。

走行体は、SPIKE-RT[^SPIKE-RT] という RTOS（Real-Time Operating System）を SPIKE に搭載しています。
プログラミング言語 C++ によって SPIKE API [^SPIKE-API] を通じてモーターの駆動や、センサーでの測定値取得などを行います。
実際の開発環境では、ET ロボコン運営が用意する RasPike-ART[^RasPike-ART] という Raspberry Pi と SPIKE の制御開発環境を通じて API を叩き、制御します。

ET ロボコンにおける各チームの開発対象は、走行体を制御するソフトウェアがほとんどです。
よって、一般的なロボコンを「ロボコン = ハードウェア開発 + ソフトウェア開発」とするならば、ET ロボコンは「半分組み込まない」ロボコンと形容できるでしょう。

[^SPIKE-RT]: GitHub, spike-rt, <https://github.com/spike-rt/spike-rt>, 最終アクセス日：2025/05/01

[^SPIKE-API]: SPIKE-RT C API Reference[Japanese], <https://spike-rt.github.io/spike-rt/ja/html/modules.html>, 最終アクセス日：2025/05/01

[^RasPike-ART]: GitHub, RasPike-ART, <https://github.com/ETrobocon/RasPike-ART>, 最終アクセス日：2025/05/01


### 総合成績が「走行」と「設計」で決定

ET ロボコン最大の特徴のひとつは、設計が成績の半分を占めることです。
大規模化・複雑化するソフトウェア開発において、ソースコードのみを追跡して開発を進めることは困難です。そこで、設計によって、それをある視点で抽象化 (**モデリング**) し、大規模化するソフトウェアの全体像が把握しやすくなります。

ET ロボコンでは、開発対象である走行体を制御するソフトウェアについて、モデリングした設計書であるモデルを運営に提出する必要があります。
いわゆる UML（Unified Modeling Language）や SysML（Systems Modeling Language）といったモデリング言語を用います。

ET ロボコンは、1 年に地区大会とチャンピオンシップ大会（全国大会）の計 2 回の大会が開催されます。
両大会において、走行部門と設計部門がそれぞれ評価され、総合成績が決まります。


## 大会の詳細とチーム KatLab の攻略方法

本章では、大会の詳細を 2024 年度開催の ET ロボコン 2024 の内容のうち、走行部門に関することを 3 つと、モデルについてを紹介いたします。
同時に、私が所属するチーム KatLab での攻略方法を紹介いたします。
特にモデルについては、言語化すると、かなりわかりにくくなっています。
読みにくいことを承知で書いています。
構造や処理のフローをモデルという形で抽象化することで、簡単にプログラム全体の理解度を上げられるということを、ぜひ実感していただきたいと考えているためです。

記述する図などは ET ロボコン実行委員会が配布している次の資料から引用いたします。
なお、モデル内では EV3API という記述があります。
これは SPIKE-API と同じような機構がある別物ですが、本稿内では区別する必要がないため、SPIKE-API で統一して記述します。

- ET ロボコン 2024 競技規約 1.0.2 版[^公式規約]
- ET ロボコン 2024 難所組立図 1.0.0 版[^難所攻略図]
- ET ロボコン 2024 審査規約 1.0 版[^審査規約]

[^公式規約]: ET ロボコン実行委員会, ET ロボコン 2024 競技規約 1.0.2 版, <https://docs.etrobo.jp/rules/2024/ETRC2024_rules_primary_advanced_1.0.2.pdf>, 最終アクセス日：2025/05/01

[^難所攻略図]: ET ロボコン実行委員会, ET ロボコン 2024 難所組立図 1.0.0 版, <https://docs.etrobo.jp/rules/2024/ETRC2024_nansyo_1.0.0.pdf>, 最終アクセス日：2025/05/01

[^審査規約]: ET ロボコン実行委員会, ET ロボコン 2024 審査規約 1.0 版, <https://docs.etrobo.jp/rules/2024/ETRC2024_dev_shinsa_rules_V1.0.pdf>, 最終アクセス日：2025/05/01


### 走行部門

ET ロボコン 2024 で走行体が走るコースの全景を、図 1 に示します。
図 1 左半分の L コースと右半分の R コースからなり、各コースにつき 1 回、計 2 回のうち高い得点の方を採用します。

![コースの全景](./images_yu_kimura/course.png 'コースの全景')

走行部門ではポイント獲得対象として、走行ポイントとボーナスポイントが獲得できます。
走行部門のポイントは、走行ポイントとボーナスポイントの合計で算出されます。

走行ポイントは、スタート（図 1 中の①）からラップゲート（図 1 中の②）を通過するまでにかかる時間によって入るポイントです。
ラップゲート通過までの時間を短縮できるほど、獲得できる走行ポイントは高くなります。

ボーナスポイントは、ラップゲート通過や難所攻略など、ある特定の条件を達成した際に獲得できるポイントです。
難所の攻略状況によって、獲得できるポイントは上下する場合があります。

なお、ポイントを算出する式や難所あたりで獲得できるポイントなどについては省略いたします。


#### 走行体の制御

KatLab は、走行体にコース上を走らせるため、ライントレースを採用しました。
プログラミング言語は、走行体の制御はリアルタイムな動作制御が必要であるため、C++ 言語を採用しました。
L コースについては、黒線の左境界を中心（左エッジ）として、走行体下部にあるカラーセンサーが白色を検知すると右前方へ、黒色を検知すると左前方へ進むよう実装しました。
対して R コースでは、黒線の右境界を中心（右エッジ）として、白色を検知すると左前方へ、黒色を検知すると右前方へ進むよう実装しました。
走行体の正面下部にあるカラーセンサーは LED ライトを備えており、コースに向かって光を放ちます。
その反射光をセンサーが読み取ることで、RGB 値を取得し、色を判定しています。
これにより、走行体は黒線の左境界を中心にジグザグと前方に進みます。
L コースを走行体がライントレースした際のイメージ図を、図 2 に示します。

![ライントレースのイメージ図（L コース）](./images_yu_kimura/line_trace.png 'ライントレースのイメージ図（L コース）')

制御方法として、 PID 制御を採用しています。
同時に、走行距離に応じて PID の各ゲインを調節できるようにしています。
これにより、直線区間とカーブ区間での曲がりやすさをそれぞれ決められます。
特にカーブ区間では、白色を検知した際の曲がりやすさを高めることで、単に速度を落とす場合に比べて、コースアウトのリスクをより効果的に低減できます。


#### プラレール・背景撮影

2 つの円のうち真円のエリア（図 1 中の③）では、真円の内側を周回する 3 両のプラレールと、四角柱の側面ひとつにある背景を収めて画像を撮影することでボーナスポイントを獲得できます。
背景のみ、プラレールのみの撮影でも一定のポイントは獲得できます。
しかし、背景とプラレール 3 両側面を同時に撮影することで、獲得できるポイントが高くなります。
プラレール・背景撮影を攻略している走行体の様子を、図 3 に示します。

![プラレール・背景撮影攻略中の走行体](./images_yu_kimura/snap_plarail.png 'プラレール・背景撮影攻略中の走行体')

KatLab は、動画の録画と動体検知によって上限の点数を獲得できました。
プログラミング言語は、Raspberry Pi のカメラモジュールについて豊富なライブラリが存在するため、 Python を採用しました。
背景の方向は当日の本番前に決定します。撮影箇所を 6 箇所パラメータとして持たせ、走行前にパラメータを設定することで背景を撮影しました。
そこで、プラレール 3 両側面を撮影するため、プラレールがカメラを通過する区間の中間のフレームを特定して切り出す方法を採用しました。

具体的には、次のように実装しました。

1. 背景をカメラ正面で捉える
1. カメラ中央にバウンディングボックスを配置する
1. 録画を開始する
1. プラレールがバウンディングボックスに侵入したタイミングを計測する
1. プラレールがバウンディングボックスを退出したタイミングを計測する
1. 計測したバウンディングボックス侵入・退出タイミングから中間のフレームを切り出す

難所組立図には、本番に用いるプラレールの画像がありませんでした。
機械学習による実装では、プラレールの種類によっては学習モデルが合わず、本番でプラレールをうまく検出できない場合があると考えました。
そこで、画像処理の中でも、プラレールの種類によらない方法である動体検知で実装しました。
バウンディングボックスにプラレールが侵入したときのフレーム、退出したときのフレーム、中間のフレームを、図 4 に示します。

![中間フレームの切り出し](./images_yu_kimura/snap_plarail_image.png '中間フレームの切り出し')


#### ミニフィグ撮影

2 つの円のうち楕円のエリア（図 1 中の④）では、中央にあるミニフィグの画像を撮影することでボーナスポイントを獲得できます。
ミニフィグ全身と台座が映っていれば、どの方向から撮影しても一定のポイントを獲得できます。
しかし、ミニフィグ正面（両目、口が判別できる状態）から撮影することで、獲得できるポイントが高くなります。
ミニフィグ撮影を攻略している走行体の様子を、図 5 に示します。

![ミニフィグ撮影攻略中の走行体](./images_yu_kimura/snap_fig.png 'ミニフィグ撮影攻略中の走行体')

KatLab は機械学習と画像処理によって上限の点数を獲得できました。
プログラミング言語は、プラレール・背景撮影と同様の理由から、Python を採用しました。
プラレール撮影の背景とは異なり、ミニフィグの向いている方向は走行直前に決まります。このタイミングは PC と走行体には触れられないため、パラメータとして設定できません。
また、本番で使用するミニフィグの種類は、難所組立図に記載されていました。
そこで、機械学習によってミニフィグが向いている方向を判定する方法を採用しました。

具体的には、次のように実装しました。

1. ミニフィグの向きを判定するため、事前に決定していた箇所からミニフィグを撮影する
1. 無線接続している PC へ走行体から撮影した画像を送信する
1. 1. で撮影した画像から、ミニフィグの向き（正面、右面、左面、背面）を機械学習で判定する
1. 判定した方向が正面である場合、1. で撮影した画像を提出する
1. 判定した方向が正面でない場合、判定した方向に応じた撮影箇所へ移動し、ミニフィグを撮影する

YOLO v5 を用いてミニフィグの画像を物体検知し、正面、右面、左面、背面の 4 つから判定します。
当初は走行体の Raspberry Pi で推論する予定でしたが、Raspberry Pi OS が 32 bit でした。
YOLO の環境構築時、Raspberry Pi OS が 64 bit でないと推論できないことに気づきました。
そこで、走行体へ無線通信している PC へ画像を送信し、PC で推論する方針に切り替えました。

また、3. の条件に入り、ミニフィグの向きを判定するために撮影した 1. の画像を提出した場合は、4. の移動と撮影はスキップする処理を実装しています。
この処理によって、3. の条件に入った場合は 2 回目の移動と撮影にかかる時間の分だけ、難所の攻略にかかる時間が短縮できます。

なんらかの理由でミニフィグの向きを判定できなかった場合は、ミニフィグが正面、右、左、背面のどの方向を向いていてもよいよう 4 つの箇所から画像を撮影します。
その後、機械学習で顔の正面らしさのスコアをそれぞれ算出し、もっとも正面らしいスコアが高い画像 1 枚を提出するようにしました。
ミニフィグの向き判定成功時の画像を、図 6 に示します。

![ミニフィグの向き判定（左から正面、右面、背面、左面）](./images_yu_kimura/predict_fig_direction.png 'ミニフィグの向き判定（左から正面、右面、背面、左面）')


### 設計部門

モデルは走行部門と異なり、詳細な点数が開示されません。
評価は A から D までの 4 段階に分かれており、それぞれに+と-の記号が付くことで、合計 12 段階の細かい評価が行われます。

モデルは次の 4 つで構成します。全体の構成は全チーム共通です。

1. 要求モデル
1. 分析モデル
1. 設計モデル
1. 制御モデル


#### 要求モデル

要求モデルでは、開発の目標と、それを達成するために必要な機能、機能に付随する品質や制約の検討がされているかが見られます。

KatLab が書いた要求モデルを、図 7 に示します。
KatLab は、開発目標と目標得点を決め、開発目標の達成のためのユースケースを定義し、それをもとに導出した要求と要素技術を要求図に起こしました。

![KatLab の要求モデル](./images_yu_kimura/model/1_requirements.png 'KatLab の要求モデル')

ミニフィグ撮影を例に挙げます。
目標とするポイントから逆算して、ミニフィグ撮影では、ミニフィグを正面から撮影した画像を提出することを目標としました。
この目標を達成するためには、カメラによる画像撮影する機能や、ミニフィグを正面から撮影した画像の提出する機能などが必要。
さらに、ミニフィグを正面から撮影した画像を提出するという機能を実装するためには、ミニフィグが正面を向いているかを判定する機能と、画像を提出する機能が必要。
このように、上位の要求を満たすために必要なことを、下位の要求として分解します。

このとき、通信失敗や撮影失敗時、安定した走行などの非機能要求を考慮する必要もあります。
ミニフィグ正面方向の判定が失敗した際は、4 つの画像から正面らしさのスコアを算出して比較し、もっともスコアの高い画像を提出するという要求は非機能要求にあたります。
この要求がない場合は、失敗そのものや失敗時の動作、安定性などを考慮しないことを示します。
この場合、品質特性における信頼性（正しく機能できる度合、異常発生時の回復のしやすさ）が足りていないため、減点されます。

また、上位要求を満たすための下位要求が足りない場合も、減点されます。
たとえば、ミニフィグが正面を向いた画像を撮影するという上位要求を、画像を撮影する、ミニフィグの向きを判定するという 2 つの下位要求に分解するとします。
しかし、この 2 つだけでは上位要求を満たしているといえません。
ミニフィグを撮影する場所へ移動するという要求が足りていません。
また、走行体の設計上、カメラモジュールは走行体の正面にあるものを画角に収めるよう設置します。
ただ前方へ進むだけでは、楕円内側にあるミニフィグを画角に収められない場合がほとんどです。
そのため、ミニフィグがある楕円内側へ回転するという要求も足りていません。
このように、ルールや走行体の設計、コース設計などを制約として、要求を分解する必要があります。


#### 分析モデル

分析モデルでは、導出した要求や制約に基づくシステム全体の構造や動作の分析ができているかが見られます。

KatLab が書いた分析モデルを、図 8 に示します。
KatLab は、競技規約からシステム分析による静的構造を導出し、コンテキスト分析で ET ロボコン全体、走行体内部、それぞれの間での関係性を示すことでソフトウェアの開発対象を示しました。
また、導出した要求と開発対象から、要素技術を構成するサブシステムを定義しました。
サブシステム間でやりとりされるデータなどの I/F（インターフェース）を定義することで、その静的構造を示しました。
さらに、サブシステム間の動的振舞いを、アクティビティ図で示しました。

![KatLab の分析モデル](./images_yu_kimura/model/2_analysis.png 'KatLab の分析モデル')

プラレール・背景撮影を例に挙げます。
要求モデルにおいて、背景の正面へ移動するという要求から、要求の分解を続けた結果、ライントレースの要素技術を導出しました。
まず、競技規約から、開発対象を走行体と PC の 2 つに定めました。
その中でもソフトウェア開発対象として、走行体に接続している Raspberry Pi と PC の 2 つに定めました。
プラレール・背景撮影に関連する要素技術から、走行全体に関わる走行システム、カメラ関連の処理を行うフロントカメラシステムなどのサブシステムが必要であるとわかりました。
他の要素技術を配置した結果、サブシステムは計 5 つに定義するとしました。

次に、サブシステム間でどのようなやりとりがあるのか、I/F を明確にして、静的構造を示します。
走行システムは、外部から RGB 値や輝度値を受け取り、SPIKE-API へモーター回転指示を渡すなどの I/F を定義します。
ここで、SPIKE-API はサブシステムとして定義しておらず、ソフトウェア開発対象ではないことを事前に明記しています。
同様にフロントカメラシステムでは、外部から映像や画像を受け取り、プラレールと背景を撮影した提出用の画像を Raspberry Pi 内の別のサブシステムへ渡すなどの I/F を定義します。

これにより、一部ですが、プラレール・背景撮影に関するサブシステムの静的構造が示せました。
最後に、サブシステム間の I/F を考慮して、動的振舞いを示します。
走行システムは、プラレールと背景を画角に収めるため、楕円内側に回転するという処理があると示します。
次の処理で、フロントカメラシステムにおいて、プラレールと背景の画像を撮影するという処理に移る。
このようにサブシステム内外を通じた処理の流れを、アクティビティ図に起こすことで、動的振舞いを示せます。


#### 設計モデル

設計モデルでは、システム全体の分析を元に要求を実現する各システムの構造と振舞いの設計が見られます。

KatLab が書いた設計モデルを図 9、図 10 にそれぞれ示します。
KatLab は、要求モデルと分析モデルから注意事項を抽出し、設計方針を決定しました。
設計方針から、開発システム全体の依存関係をパッケージ図で示し、アーキテクチャを決定しました（図 9）。

![KatLab の分析モデル（ 1 枚目）](./images_yu_kimura/model/3_design1.png 'KatLab の分析モデル（ 1 枚目）')

また、アーキテクチャを考慮しつつクラス図によって各サブシステムの内部構造と関連性を記述し、状態遷移図とアクティビティ図を用いて動的振舞いを示しました（図 10）。

![KatLab の分析モデル（ 2 枚目）](./images_yu_kimura/model/3_design2.png 'KatLab の分析モデル（ 2 枚目）')

プラレール・背景撮影、および、ミニフィグ撮影に関して、Facade（ファサード）パターンを例に挙げます。
開発効率について、拡張性を向上するという上位要求を導出しました。
さらにその上位要求から、パッケージ間を疎結合にするという下位要求を導出し、Facade パターンの設計方針を要求モデルで導出しました。
Facade パターンを実現するため、今年の競技固有の機能を束ねたパッケージと、汎用的な機能を束ねたパッケージに分解しました。
たとえば、カメラを用いて画像や動画を撮影する処理は、汎用的な処理にあたります。
対して、プラレールのバウンディングボックス侵入・退出タイミングを用いた中間フレーム切り出しの処理や、ミニフィグの向きを判定する処理は、今年の競技固有の処理にあたります。
設計方針において、カメラに関するサブシステムであるフロントカメラシステムは、次の 4 つのパッケージに分解しました。

- カメラインターフェース
- 動体検出部
- 画像解析部
- カメラ制御部

動体検出部と画像解析部は、プラレールの中間フレーム切り出しや、ミニフィグの向き判定などの競技固有の処理をそれぞれ含んでいます。
カメラ制御部は、画像や動画の撮影などの汎用的な処理を含んでいます。
動体検出部、画像解析部はカメラ制御部の public な関数を呼び出しています。
さらに動体検出部と画像解析部の各パッケージにおける処理は、カメラインターフェースで定義した public な関数を経由して呼び出されます。
これによって、フロントカメラシステムにおける保守性と拡張性の向上という非機能要求を実現可能な設計に落とし込みます。

#### 制御モデル

制御モデルは、要求で定義した品質を満たすための制御戦略と、その戦略で用いられる要素技術についての検討内容と分析結果が見られます。

KatLab が書いた制御モデルを、図 11 に示します。
KatLab は、走行全体、プラレール・背景撮影、ミニフィグ撮影について制御戦略が要求を満たすことを確認しました。
走行体の動作を細分化し、図を含めて制御戦略の説明を示しました。
詳細なデータを示せる場合は具体的に、成功率 ◯◯ ％ 達成などと数値を用いて、採用した制御戦略が実現可能であることを定量的に評価しました。

![KatLab の制御モデル](./images_yu_kimura/model/4_control.png 'KatLab の制御モデル')

プラレール・背景撮影を例に挙げます。
競技規約から、真円の内側に背景があることと、背景の向きは、本番当日の走行前に決定することがわかっていました。
真円という形状から、円周上に等間隔で 6 つの撮影箇所を設けることで、6 つの撮影箇所のうちどこか 1 箇所は背景を撮影できると考えました。
しかし、ミニフィグの向きと同様に、プラレールの周回方向は走行直前に決定します。
このタイミングは PC と走行体には触れられないため、パラメータとして設定できません。
そのため、プラレールの周回方向に依存しないような方法でプラレール撮影の機能を実装する必要があるとしました。
ここで、プラレールがバウンディングボックスに侵入したタイミングと、退出したタイミングの中間フレームを切り出す実装を説明しています。

また、競技規約より、背景下部には黄色の矩形があるとわかっていました。
そこで、黄色の矩形がカメラの画角正面に収まるよう、回転の角度を補正する処理を実装しました。
背景をより正面に捉えられるようになったため、この難所における上限のポイントを獲得できる確率が 90 ％以上になったと定量的に示しました。

制御モデルは各チームによってかなり差があります。
KatLab は代々、ユニークなツールを作成したことを書いています。
今年は、走行ログ可視化ツールを Web アプリとして作成したことで、走行動作が失敗する原因の特定に寄与したと、走行全体の制御戦略のひとつとして書きました。
昨年は、パラメータ指定ツールを iOS アプリとして作成したことで、調整にかかる時間を削減できたと書きました。

## 2024年度のチーム KatLab での体験

本章では、チーム KatLab で得られた体験について書きます。

### 大会成績

九州地区大会でモデル 1 位、総合 2 位。
チャンピオンシップ大会（全国大会）で総合 5 位の成績を収めました。
KatLab は 2018 年から 7 年連続で全国出場を果たしていたため、その記録を継続させることができました。


### チームリーダーの経験

昨年と同じく、開発方式はアジャイル開発で進めました。
モデルを書くのであればウォーターフォール開発が向いていそうとよくいわれますが、これはイテレーションごとに動作するものを作り上げられるためです。
ウォーターフォール開発は、未完成の段階では動作が保証できません。
一方でアジャイル開発は、イテレーションごとに動くものを細かく作成します。
ET ロボコンのコースは、難所の攻略順がおおよそ決まっています。
そのため、途中で動作が止まる、コースアウトするなどで復帰不能となった場合は、以降の得点がすべて失われます。
極端な話、ラップゲート通過前にコースアウトすると、ボーナスポイントはもちろん走行ポイントも入りません。
まず基礎的な動作を完成させ、次はこの難所攻略、次はこの難所攻略、と段階を踏んで開発することで、開発が間に合わなかった場合に大きく失点するリスクを防いでいます。

マネジメント関連は、常に自分からコミュニケーションを取りにいくことを意識しました。
タスクで困っていることがないか、実装方法がミーティング時から変わったことがないかを積極的に確認していました。
ただ、すべてを把握しようとすると窮屈に思われると考えたため、タスクにつき 1 回や 2 回と意識的に抑えていました。
メンバーが全員優秀だったこともあり、明らかに困っている様子でない限り、タスクごとに分けたグループ内で解決してもらう方針を取りました。
そのため、学生プログラミング団体でありがちな、一部の優れた人間が開発を先導する状況にはなりませんでした。
これは、平等に学習の機会があったという意味でよい効果があったと考えています。


### 感想と2025年度の目標

学生のうちから設計に関わることはかなり珍しいと思っています。
それだけに、今しか集まれないメンバーと足並みを揃えて、今しかできない ET ロボコンで得た経験は、自分を形成する上で欠かせないものだと強く考えています。
こうしてゆめみに内定をもらったり、ゆめみ大技林に寄稿することもなかったことでしょう。
優秀なメンバーに囲まれ、リーダーとして特別にやったことはさほどなかったと記憶しています。
いくつも失敗しましたが、それでもメンバーのみんながフォローしてくれ、最後まで開発できたので大変満足しています。
メンバーのみんなには本当に頭が上がりません。

今年度も全国大会にて総合 3 位以内で入賞の目標を継続します。
KatLab は 2018 年から 7 年連続で全国出場を果たしているものの、全国での成績が停滞気味です。
今年度こそ、新生 KatLab メンバーで全国入賞を果たします。


## まとめ

まず、ゆめみ大技林の執筆について勧誘をしていただきました、うーたん様にお礼申し上げます。
ゆめみ大技林の執筆者の方々にも、レビューと推敲をしていただきました。ありがとうございます。
KatLab のチームメンバーの皆さんも、推敲を手伝ってくださりありがとうございました。
昨年活動分の感謝と一緒に、今年の ET ロボコンの活動で返します。

最後に、ここまで閲覧いただきありがとうございます。
アプリというより組み込み、プログラムというより設計と、他の執筆者の方々とは少し外れてしまったテーマでした。
しかし、希少性のあるテーマで寄稿できたため、よいことだと前向きに捉えています。

本文で用いたモデルの画像は、Google Drive に共有フォルダとしてまとめました。
拡大してモデル画像を見たい方は、次のリンク、もしくは QR コードからご覧ください。
モデルの画像をまとめた Google Drive の QR コードを、図 12 に示します。

```url
https://drive.google.com/drive/folders/1r0xD7jiTnK6KrH1i7InL1uyFjWCyJ-F2?usp=drive_link
```

![モデルの画像を集めた Google Drive の QR コード](./images_yu_kimura/google_drive_qr_code.png 'モデルの画像を集めた Google Drive の QR コード')
